{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1725a8153cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCSVLogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_generator'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_generator.batch_generator import BatchGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\n",
    "from models import AlexNet, LeNet\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image-treat-1: sem data augmentation, com normalização e equalização\n",
    "\n",
    "Image-treat-2: com data augmentation, com normalização e equalização\n",
    "\n",
    "Image-treat-3: com data augmentation, com normalização e com equalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('callbacks/lenet/age/history-regression-image-treat-2-lrelu.csv',\n",
       " 'callbacks/lenet/age/class-weights-image-treat-2-lrelu.{epoch:02d}-{val_loss:.2f}.hdf5')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approach = 'image-treat-2' \n",
    "activation = 'relu'\n",
    "net = 'alexnet'\n",
    "\n",
    "if net == 'alexnet':\n",
    "    model = AlexNet\n",
    "elif net =='lenet':\n",
    "    model = LeNet\n",
    "csvlogger_name = 'callbacks/'+net +'/age/history-regression-' + approach + '-' + activation + '.csv'\n",
    "checkpoint_filename = 'callbacks/'+net+'/age/class-weights-' + approach + '-' + activation + '.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "csvlogger_name, checkpoint_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/nicoli/github/alexnet/dataset/csv/imdb_csv/imdb_age_regression_train_split_47950-70-10-20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df.columns[1:])\n",
    "in_format = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BatchGenerator(box_output_format=cols)\n",
    "validation_dataset = BatchGenerator(box_output_format=cols)\n",
    "\n",
    "train_dataset. parse_csv(labels_filename='/home/nicoli/github/alexnet/dataset/csv/imdb_csv/imdb_age_regression_train_split_47950-70-10-20.csv', \n",
    "                        images_dir='/home/nicoli/github/alexnet/dataset/imdb-hand-crop',\n",
    "                        input_format=in_format)\n",
    "\n",
    "validation_dataset.parse_csv(labels_filename='/home/nicoli/github/alexnet/dataset/csv/imdb_csv/imdb_age_regression_val_split_47950-70-10-20.csv', \n",
    "                             images_dir='/home/nicoli/github/alexnet/dataset/imdb-hand-crop',\n",
    "                             input_format=in_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width, img_depth = (224,224,3)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "train_batch_size = 32\n",
    "shuffle = True\n",
    "ssd_train = False\n",
    "\n",
    "validation_batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the dataset: 33565\n",
      "Number of images in the dataset: 4795\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_dataset.generate(batch_size=train_batch_size,\n",
    "                                         shuffle=shuffle,\n",
    "                                         ssd_train=ssd_train,\n",
    "                                         random_rotation=20,\n",
    "                                         translate=(0.2, 0.2),\n",
    "                                         scale=(0.8, 1.2),\n",
    "                                         flip=0.5,\n",
    "                                         divide_by_stddev=255,\n",
    "                                         returns={'processed_labels'},\n",
    "                                         resize=(img_height, img_width))\n",
    "\n",
    "validation_generator = validation_dataset.generate(batch_size=validation_batch_size,\n",
    "                                                   shuffle=shuffle,\n",
    "                                                   ssd_train=ssd_train,\n",
    "                                                   divide_by_stddev=255,\n",
    "                                                   returns={'processed_labels'},\n",
    "                                                   resize=(img_height, img_width))\n",
    "\n",
    "print(\"Number of images in the dataset:\", train_dataset.get_n_samples())\n",
    "print(\"Number of images in the dataset:\", validation_dataset.get_n_samples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = train_dataset.get_n_samples()/train_batch_size\n",
    "validation_steps = validation_dataset.get_n_samples()/validation_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicoli/github/alexnet/models.py:109: UserWarning: Considering a regression task with output function being lrelu\n",
      "  warnings.warn(warning)\n"
     ]
    }
   ],
   "source": [
    "model = model(n_classes=1, img_width=img_width, img_depth=img_depth, img_height=img_height, activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_1 (Conv2D)              (None, 220, 220, 6)       456       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 220, 220, 6)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 110, 110, 6)       0         \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv2D)              (None, 106, 106, 16)      2416      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 106, 106, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 53, 53, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 44944)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               5393400   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 84)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 85        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 5,406,521\n",
      "Trainable params: 5,406,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.00001, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger(csvlogger_name, append=True, separator=',')\n",
    "\n",
    "checkpoint = ModelCheckpoint(checkpoint_filename,\n",
    "                           monitor='val_loss',\n",
    "                           verbose=1,\n",
    "                           save_best_only=False,\n",
    "                           period=1)\n",
    "\n",
    "earlystopping = EarlyStopping(patience=5, mode='min')\n",
    "\n",
    "#callbacks = [tensorboard, checkpoint]\n",
    "callbacks=[checkpoint, csv_logger, earlystopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1049/1048 [==============================] - 3751s 4s/step - loss: 223.4216 - mean_absolute_error: 11.7186 - val_loss: 198.9249 - val_mean_absolute_error: 11.1937\n",
      "\n",
      "Epoch 00001: saving model to callbacks/lenet/age/class-weights-image-treat-2-lrelu.01-198.92.hdf5\n",
      "Epoch 2/100\n",
      "1049/1048 [==============================] - 3712s 4s/step - loss: 207.4345 - mean_absolute_error: 11.3573 - val_loss: 193.5887 - val_mean_absolute_error: 10.9629\n",
      "\n",
      "Epoch 00002: saving model to callbacks/lenet/age/class-weights-image-treat-2-lrelu.02-193.59.hdf5\n",
      "Epoch 3/100\n",
      " 601/1048 [================>.............] - ETA: 25:11 - loss: 204.8360 - mean_absolute_error: 11.2909"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator, epochs=epochs, \n",
    "                             steps_per_epoch=steps_per_epoch, \n",
    "                             validation_data=validation_generator,\n",
    "                             validation_steps=validation_steps,\n",
    "                             callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
